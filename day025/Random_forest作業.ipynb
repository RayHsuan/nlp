{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 作業目的: 了解何謂Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: 一般模型誤差可拆為三部分，這三部分的誤差分別是？\n",
    "\n",
    "Answer: \n",
    "1.bias error (error from wrong model assumptions)\n",
    "2.variance (error from sensitivity to small fluctuations in training data)\n",
    "3.irreducible error (inherent noise in the problem itself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2: 請解釋何謂Bias-Variance Tradeoff\n",
    "\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our model suffers from high bias, the average response of the model is far from the true value and we call this underfitting. When our model suffers from high variance, this is usually a result of its inability to generalize well beyond the training data and we call this overfitting. Our goal is to build a model that achieves a balance between bias and variance so that the combined error of these two competing forces is minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "扣除掉數據本身的誤差，也就是irreducible error，我們必須要在模型中提高泛化能力，卻又不能讓模型過度學習而失去準度，當我們需要降低Bias error和Variance error時，這兩者相對應的做法剛好相反。\n",
    "overfitting指的是增加模型複雜度(特徵)，變異性就會上升，反過來為了將低變異性而降低模型複雜度，卻會導致underfitting(bias error上升)\n",
    "所以說這是一個取平衡點的Tradeoff..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='bias_variance_tradeoff.png' style='width:500px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機器學習的基本問題是利用模型對數據進行擬合，學習的目的並非是對有限訓練集進行正確預測，而是對未曾在訓練集合出現的樣本能夠正確預測。模型對訓練集數據的誤差稱為經驗誤差，對測試集數據的誤差稱為泛化誤差。模型對訓練集以外樣本的預測能力就稱為模型的泛化能力，追求這種泛化能力始終是機器學習的目標。\n",
    "\n",
    "1.過擬合(overfitting)和欠擬合(underfitting)的概念\n",
    "\n",
    "過擬合(overfitting)和欠擬合(underfitting)是導致模型泛化能力不高的兩種常見原因，都是模型學習能力與數據復雜度之間失配的結果。\n",
    "\n",
    "過擬合(overfitting)：“過擬合”常常在模型學習能力過強的情況中出現，此時的模型學習能力太強，以至於將訓練集單個樣本自身的特點都能捕捉到，並將其認為是“一般規律”，同樣這種情況也會導致模型泛化能力下降。\n",
    "\n",
    "欠擬合(underfitting)：“欠擬合”常常在模型學習能力較弱，而數據復雜度較高的情況出現，此時模型由於學習能力不足，無法學習到數據集中的“一般規律”，因而導致泛化能力弱。\n",
    "\n",
    "兩者的區別：欠擬合在訓練集和測試集上的性能都較差，而過擬合往往能較好地學習訓練集數據的性質，而在測試集上的性能較差。在神經網絡訓練的過程中，欠擬合主要表現為輸出結果的高偏差，而過擬合主要表現為輸出結果的高方差。\n",
    "\n",
    "2.兩個概念的圖示\n",
    "???\n",
    "\n",
    "\n",
    "3.產生的原因\n",
    "\n",
    "欠擬合(underfitting)：（1）模型復雜度過低 （2）特征量過少\n",
    "\n",
    "過擬合(overfitting)：\n",
    "\n",
    "（1）建模樣本選取有誤，如樣本數量太少，選樣方法錯誤，樣本標簽錯誤等，導致選取的樣本數據不足以代表預定的分類規則\n",
    "\n",
    "（2）樣本噪音幹擾過大，使得機器將部分噪音認為是特征從而擾亂了預設的分類規則\n",
    "\n",
    "（3）假設的模型無法合理存在，或者說是假設成立的條件實際並不成立\n",
    "\n",
    "（4）參數太多，模型復雜度過高\n",
    "\n",
    "（5）對於決策樹模型，如果我們對於其生長沒有合理的限制，其自由生長有可能使節點只包含單純的事件數據(event)或非事件數據(no event)，使其雖然可以完美匹配（擬合）訓練數據，但是無法適應其他數據集\n",
    "\n",
    "（6）對於神經網絡模型：a)對樣本數據可能存在分類決策面不唯一，隨著學習的進行,，BP算法使權值可能收斂過於復雜的決策面；b)權值學習叠代次數足夠多(Overtraining)，擬合了訓練數據中的噪聲和訓練樣例中沒有代表性的特征\n",
    "\n",
    "4.解決方案\n",
    "\n",
    "解決欠擬合(underfitting)：\n",
    "\n",
    "（1）增加新特征，可以考慮加入進特征組合、高次特征，來增大假設空間\n",
    "\n",
    "（2）添加多項式特征，這個在機器學習算法裏面用的很普遍，例如將線性模型通過添加二次項或者三次項使模型泛化能力更強\n",
    "\n",
    "（3）減少正則化參數，正則化的目的是用來防止過擬合的，但是模型出現了欠擬合，則需要減少正則化參數\n",
    "\n",
    "（4）使用非線性模型，比如核SVM 、決策樹、深度學習等模型\n",
    "\n",
    "（5）調整模型的容量(capacity)，通俗地，模型的容量是指其擬合各種函數的能力\n",
    "\n",
    "（6）容量低的模型可能很難擬合訓練集；使用集成學習方法，如Bagging ,將多個弱學習器Bagging\n",
    "\n",
    "解決過擬合(overfitting)：\n",
    "\n",
    "（1）正則化（Regularization）（L1和L2）\n",
    "\n",
    "在模型訓練的過程中，需要降低 loss 以達到提高 accuracy 的目的。此時，使用正則化之類的方法直接將權值的大小加入到 loss 裏，在訓練的時候限制權值變大。訓練過程需要降低整體的 loss，這時候，一方面能降低實際輸出與樣本之間的誤差，也能降低權值大小正則化方法包括 L0 正則、 L1正則和 L2 正則，而正則一般是在目標函數之後加上範數。L2 範數是指向量各元素的平方和然後求平方根。可以使得 WW 的每個元素都很小，都接近於0，但不會讓它等於0，而是接近於0。 L2L2正則項起到使得參數 WW 變小加劇的效果，關於它為什麽能防止過擬合簡答的理解為：更小的參數值 W意味著模型的復雜度更低，對訓練數據的擬合剛剛好，不會過分擬合訓練數據，從而使得不會過擬合，以提高模型的泛化能力。\n",
    "\n",
    "（2）數據擴增，即增加訓練數據樣本\n",
    "\n",
    "這是解決過擬合最有效的方法，只要給足夠多的數據，讓模型「看見」盡可能多的「例外情況」，它就會不斷修正自己，從而得到更好的結果。\n",
    "\n",
    "那麽問題來了，我們如何獲取更多的數據？以下有這幾種辦法：\n",
    "\n",
    "　　1）從數據源頭獲取更多數據\n",
    "\n",
    "　　2）根據當前數據集估計數據分布參數，使用該分布產生更多數據：這個一般不用，因為估計分布參數的過程也會代入抽樣誤差\n",
    "\n",
    "　　3）數據增強（Data Augmentation）：通過一定規則擴充數據。如在物體分類問題裏，物體在圖像中的位置、姿態、尺度，整體圖片明暗度等都不會影響分類結果。我們就可以通過圖像平移、翻轉、縮放、切割等手段將數據庫成倍擴充\n",
    "\n",
    "（3）Dropout\n",
    "\n",
    "在訓練時，每次隨機（如50%概率）忽略隱層的某些節點；這樣，我們相當於隨機從 2n2n(n個神經元的網絡) 個模型中采樣選擇模型\n",
    "\n",
    "（4）Early stopping\n",
    "\n",
    "Early stopping便是一種叠代次數截斷的方法來防止過擬合的方法，即在模型對訓練數據集叠代收斂之前停止叠代來防止過擬合。具體做法是，在每一個Epoch結束時計算validation data的accuracy，當accuracy不再提高時，就停止訓練。當然我們並不會在accuracy一降低的時候就停止訓練，因為可能經過這個Epoch後，accuracy降低了，但是隨後的Epoch又讓accuracy又上去了，所以不能根據一兩次的連續降低就判斷不再提高。一般的做法是，在訓練的過程中，記錄到目前為止最好的validation accuracy，當連續10次Epoch（或者更多次）沒達到最佳accuracy時，則可以認為accuracy不再提高了。此時便可以停止叠代了（Early Stopping）。這種策略也稱為“No-improvement-in-n”，n即Epoch的次數，可以根據實際情況取，如10、20、30……"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
